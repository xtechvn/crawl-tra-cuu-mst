import aio_pika
import json
import logging
import asyncio
from config import config
from crawler.tax_crawler import TaxCrawler
from redis import get_redis
import time

logger = logging.getLogger(__name__)

class TaxCrawlConsumer:
    def __init__(self):
        self.connection = None
        self.channel = None
        self.queue = None
        self.exchange = None
        self.result_queue = None
        self.crawler = TaxCrawler()

    async def connect(self):
        """K·∫øt n·ªëi ƒë·∫øn RabbitMQ"""
        try:
            self.connection = await aio_pika.connect_robust(config.RABBITMQ_URL)
            self.channel = await self.connection.channel()
            
            # Khai b√°o exchange
            self.exchange = await self.channel.declare_exchange(
                config.RABBITMQ_EXCHANGE,
                aio_pika.ExchangeType.TOPIC
            )
            
            # Khai b√°o input queue
            self.queue = await self.channel.declare_queue(
                config.RABBITMQ_QUEUE_NAME,
                durable=True
            )
            
            # Khai b√°o result queue
            self.result_queue = await self.channel.declare_queue(
                config.RABBITMQ_RESULT_QUEUE,
                durable=True
            )
            
            # Bind queues v·ªõi exchange
            await self.queue.bind(
                self.exchange,
                routing_key="tax.crawl"
            )
            
            await self.result_queue.bind(
                self.exchange,
                routing_key="tax.result"
            )
            
            logger.info("‚úÖ RabbitMQ Consumer connected successfully")
            logger.info(f"üìã Input Queue: {config.RABBITMQ_QUEUE_NAME}")
            logger.info(f"üìã Result Queue: {config.RABBITMQ_RESULT_QUEUE}")
            logger.info(f"üìã Exchange: {config.RABBITMQ_EXCHANGE}")
            logger.info(f"üìã Routing Keys: tax.crawl -> tax.result")
            
        except Exception as e:
            logger.error(f"‚ùå RabbitMQ Consumer connection failed: {e}")
            raise

    async def process_message(self, message: aio_pika.IncomingMessage):
        """X·ª≠ l√Ω message t·ª´ queue"""
        async with message.process():
            try:
                # Parse message
                body = json.loads(message.body.decode())
                tax_no = body.get("tax_no")
                request_id = body.get("request_id")
                
                logger.info(f"üîÑ Processing crawl task: tax_no={tax_no}, request_id={request_id}")
                logger.info(f"üì• Raw message body: {message.body.decode()}")
                
                # Th·ª±c hi·ªán crawl
                start_time = time.time()
                status, data = await self.crawler.search_by_tax_code(tax_no)
                end_time = time.time()
                elapsed_time = end_time - start_time
                
                # Cache k·∫øt qu·∫£ cho t·∫•t c·∫£ tr∆∞·ªùng h·ª£p (success, no data, error)
                redis = await get_redis()
                cache_key = f"{tax_no}"
                
                # L∆∞u data ho·∫∑c error message
                cache_data = data if status == 0 else {"status": status, "message": "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu" if status == 1 else str(data)}
                await redis.set(cache_key, json.dumps(cache_data), ex=43200)  # 12 ti·∫øng
                logger.info(f"üíæ Cached result for tax_no: {tax_no}, status: {status}")
                
                # T·∫°o result message
                result = {
                    "request_id": request_id,
                    "tax_no": tax_no,
                    "status": status,
                    "data": data if status == 0 else None,
                    "crawl_time_seconds": elapsed_time,
                    "timestamp": time.time()
                }
                
                # G·ª≠i k·∫øt qu·∫£ v·ªÅ result queue
                await self.exchange.publish(
                    aio_pika.Message(
                        body=json.dumps(result).encode(),
                        delivery_mode=aio_pika.DeliveryMode.PERSISTENT,
                        headers={"request_id": request_id}
                    ),
                    routing_key="tax.result"
                )
                
                logger.info(f"‚úÖ Completed crawl task: tax_no={tax_no}, status={status}, time={elapsed_time:.2f}s")
                logger.info(f"üì§ Sending result to result queue: {result}")
                
            except Exception as e:
                logger.error(f"‚ùå Error processing message: {e}")
                # G·ª≠i error result
                error_result = {
                    "request_id": body.get("request_id") if 'body' in locals() else "unknown",
                    "tax_no": body.get("tax_no") if 'body' in locals() else "unknown",
                    "status": 2,
                    "error": str(e),
                    "timestamp": time.time()
                }
                
                await self.exchange.publish(
                    aio_pika.Message(
                        body=json.dumps(error_result).encode(),
                        delivery_mode=aio_pika.DeliveryMode.PERSISTENT
                    ),
                    routing_key="tax.result"
                )

    async def start_consuming(self):
        """B·∫Øt ƒë·∫ßu consume messages"""
        if not self.connection:
            await self.connect()
        
        logger.info(f"üéØ Starting consumer for queue: {config.RABBITMQ_QUEUE_NAME}")
        
        # B·∫Øt ƒë·∫ßu consume
        await self.queue.consume(self.process_message)
        
        try:
            # Gi·ªØ consumer ch·∫°y
            await asyncio.Future()  # Ch·∫°y v√¥ h·∫°n
        except KeyboardInterrupt:
            logger.info("üõë Consumer stopped by user")
        finally:
            await self.close()

    async def close(self):
        """ƒê√≥ng k·∫øt n·ªëi"""
        if self.connection:
            await self.connection.close()
            logger.info("üîå RabbitMQ Consumer connection closed")

# Consumer instance
consumer = TaxCrawlConsumer() 